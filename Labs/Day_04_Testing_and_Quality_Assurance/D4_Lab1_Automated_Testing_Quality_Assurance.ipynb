{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the source code for our main application from `app/main.py`. Providing the full code as context is essential for the LLM to generate accurate and relevant tests.\n",
    "\n",
    "**Model Selection:**\n",
    "For generating tests, models with strong code understanding and logical reasoning are best. `gpt-4.1`, `o3`, `codex-mini`, and `gemini-2.5-pro` are all excellent choices for this task.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated test files.\n",
    "- `clean_llm_output()`: To clean up the generated Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\labadmin\\Desktop\\AISWE\\AG-AISOFTDEV\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'huggingface' with model 'deepseek-ai/DeepSeek-V3.1'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body).\n",
    "4.  Generate another test for the `GET /users/` endpoint.\n",
    "5.  Save the generated tests into a file named `tests/test_main_simple.py`.\n",
    "\n",
    "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "# tests/test_main_simple.py\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from sqlalchemy.pool import StaticPool\n",
      "\n",
      "from main import app, get_db\n",
      "from models import Base, User\n",
      "\n",
      "# Configure test database\n",
      "TEST_DATABASE_URL = \"sqlite:///:memory:\"\n",
      "engine = create_engine(\n",
      "    TEST_DATABASE_URL,\n",
      "    connect_args={\"check_same_thread\": False},\n",
      "    poolclass=StaticPool,\n",
      ")\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "\n",
      "# Override get_db dependency\n",
      "def override_get_db():\n",
      "    try:\n",
      "        db = TestingSessionLocal()\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "app.dependency_overrides[get_db] = override_get_db\n",
      "\n",
      "# Test client\n",
      "client = TestClient(app)\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def test_db():\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    yield\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "def test_create_user(test_db):\n",
      "    \"\"\"Test user creation endpoint\"\"\"\n",
      "    user_data = {\n",
      "        \"name\": \"John Doe\",\n",
      "        \"email\": \"john.doe@example.com\",\n",
      "        \"role\": \"New Hire\"\n",
      "    }\n",
      "    \n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    \n",
      "    assert response.status_code == 200\n",
      "    assert response.json()[\"email\"] == user_data[\"email\"]\n",
      "    assert response.json()[\"name\"] == user_data[\"name\"]\n",
      "    assert response.json()[\"role\"] == user_data[\"role\"]\n",
      "    assert \"id\" in response.json()\n",
      "\n",
      "def test_get_users(test_db):\n",
      "    \"\"\"Test users retrieval endpoint\"\"\"\n",
      "    # First create test users\n",
      "    users_data = [\n",
      "        {\"name\": \"User One\", \"email\": \"user1@example.com\", \"role\": \"New Hire\"},\n",
      "        {\"name\": \"User Two\", \"email\": \"user2@example.com\", \"role\": \"HR Manager\"}\n",
      "    ]\n",
      "    \n",
      "    for user_data in users_data:\n",
      "        client.post(\"/users/\", json=user_data)\n",
      "    \n",
      "    # Test retrieving all users\n",
      "    response = client.get(\"/users/\")\n",
      "    \n",
      "    assert response.status_code == 200\n",
      "    assert len(response.json()) == 2\n",
      "    assert response.json()[0][\"email\"] == users_data[0][\"email\"]\n",
      "    assert response.json()[1][\"email\"] == users_data[1][\"email\"]\n",
      "\n",
      "def test_get_users_pagination(test_db):\n",
      "    \"\"\"Test users endpoint with pagination\"\"\"\n",
      "    # Create multiple users\n",
      "    for i in range(5):\n",
      "        user_data = {\n",
      "            \"name\": f\"User {i}\",\n",
      "            \"email\": f\"user{i}@example.com\",\n",
      "            \"role\": \"New Hire\"\n",
      "        }\n",
      "        client.post(\"/users/\", json=user_data)\n",
      "    \n",
      "    # Test skip and limit parameters\n",
      "    response = client.get(\"/users/?skip=2&limit=2\")\n",
      "    \n",
      "    assert response.status_code == 200\n",
      "    assert len(response.json()) == 2\n",
      "    assert response.json()[0][\"name\"] == \"User 2\"\n",
      "    assert response.json()[1][\"name\"] == \"User 3\"\n",
      "✅ Successfully saved artifact to: tests/test_main_simple.py\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate happy path tests for your API.\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "ROLE: you are a senior QA engineer.\n",
    "TASK: write test for a FastAPI application using pytest.\n",
    "\n",
    "The application code is below. Write tests that cover the main functionality of the application.\n",
    "test 1 tests the user creation endpoint:\n",
    "generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body)\n",
    "\n",
    "test 2  tests the user retrieval endpoint:\n",
    "test for the `GET /users/` endpoint\n",
    "\n",
    "application code:\n",
    "{app_code}\n",
    "\n",
    "Save the generated tests into a file named `tests/test_main_simple.py`\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Task:** Prompt the LLM to generate tests for common edge cases, such as providing invalid data or requesting a non-existent resource.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to write two new test functions:\n",
    "    * A test for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "    * A test for the `GET /users/{user_id}` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "**Expected Quality:** Two new `pytest` functions that verify the application handles common error scenarios correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from sqlalchemy.pool import StaticPool\n",
      "\n",
      "# Import your FastAPI app\n",
      "from main import app, Base, get_db\n",
      "\n",
      "# Test database setup\n",
      "TEST_DATABASE_URL = \"sqlite:///:memory:\"\n",
      "engine = create_engine(\n",
      "    TEST_DATABASE_URL,\n",
      "    connect_args={\"check_same_thread\": False},\n",
      "    poolclass=StaticPool\n",
      ")\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "\n",
      "# Override the dependency\n",
      "def override_get_db():\n",
      "    try:\n",
      "        db = TestingSessionLocal()\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "app.dependency_overrides[get_db] = override_get_db\n",
      "\n",
      "# Test 1: Create user with duplicate email\n",
      "def test_create_user_duplicate_email():\n",
      "    # Create tables\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    \n",
      "    client = TestClient(app)\n",
      "    \n",
      "    # First user creation\n",
      "    user_data = {\n",
      "        \"email\": \"test@example.com\",\n",
      "        \"name\": \"Test User\",\n",
      "        \"role\": \"New Hire\"\n",
      "    }\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code == 200\n",
      "    \n",
      "    # Second user with same email\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code == 400\n",
      "    assert response.json()[\"detail\"] == \"Email already registered\"\n",
      "    \n",
      "    # Cleanup\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "# Test 2: Get non-existent user\n",
      "def test_get_nonexistent_user():\n",
      "    # Create tables\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    \n",
      "    client = TestClient(app)\n",
      "    \n",
      "    # Try to get user with ID 999\n",
      "    response = client.get(\"/users/999\")\n",
      "    assert response.status_code == 404\n",
      "    assert response.json()[\"detail\"] == \"User not found\"\n",
      "    \n",
      "    # Cleanup\n",
      "    Base.metadata.drop_all(bind=engine)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate edge case tests.\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "ROLE: you are a senior QA engineer.\n",
    "TASK: write test for a FastAPI application using pytest.\n",
    "\n",
    "The application code is below. Write tests that cover the main functionality of the application.\n",
    "test 1 tests the user creation endpoint:\n",
    "generate a `pytest` test function for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "\n",
    "test 2  tests the user retrieval endpoint:\n",
    "generate a `pytest` test function for the `GET /users/` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "application code:\n",
    "{app_code}\n",
    "\n",
    "Output only the raw Python code for these two test functions.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
    "\n",
    "> **Hint:** Why use an isolated database? Running tests against your actual development database can lead to data corruption and flaky, unreliable tests. A pytest fixture that creates a fresh, in-memory database for each test ensures that your tests are independent, repeatable, and have no side effects.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to generate a `pytest` fixture.\n",
    "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "4.  Crucially, it must override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
    "5.  Save the generated fixture code to a special file named `tests/conftest.py`.\n",
    "6.  Finally, create a new test file, `tests/test_main_with_fixture.py`, and ask the LLM to rewrite the happy-path tests from Challenge 1 to use the new database fixture.\n",
    "\n",
    "**Expected Quality:** Two new files, `tests/conftest.py` and `tests/test_main_with_fixture.py`, containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Pytest DB Fixture ---\n",
      "We are going to create two files: conftest.py and test_main_with_fixture.py.\n",
      " The conftest.py will set up the test database and fixtures.\n",
      " The test_main_with_fixture.py will contain the tests using the fixtures.\n",
      "\n",
      " We note that the application code is provided in the same file (main.py) and we are going to import from it.\n",
      "\n",
      " However, note that the provided application code is in a single block. We must structure our test files to work with that.\n",
      "\n",
      " Since the application code defines:\n",
      "   - Base (declarative base)\n",
      "   - User and OnboardingTask models\n",
      "   - app (FastAPI instance)\n",
      "   - get_db dependency\n",
      "\n",
      " We will import these in our conftest.py and test_main_with_fixture.py.\n",
      "\n",
      " Steps for conftest.py:\n",
      "   1. Create an in-memory SQLite database engine.\n",
      "   2. Create a SessionLocal for tests.\n",
      "   3. Create a fixture for the database session that creates the tables before and drops them after (or use transactions and rollback).\n",
      "   4. Override the get_db dependency to use the test session.\n",
      "   5. Provide a client fixture that uses the overridden app.\n",
      "\n",
      " We choose function scope for isolation. We will create and drop the tables for each test function.\n",
      "\n",
      " However, note that creating and dropping tables for every test might be slow if we have many tests.\n",
      " Alternatively, we can use transactions and rollback after each test. But since we are using SQLite in-memory, \n",
      " and the number of tests is small, we can use the create_all and drop_all approach.\n",
      "\n",
      " Let's design the fixtures:\n",
      "\n",
      "   - engine: create an in-memory SQLite engine (session scope, so we create it once for the test session)\n",
      "   - Base.metadata.create_all and drop_all: we can do this in a fixture that runs per test (function scope) or per session.\n",
      "\n",
      " But note: if we drop the tables, we lose the schema. So we must create the tables for each test if we drop them after.\n",
      "\n",
      " Alternatively, we can use a transaction and rollback after each test. However, SQLite doesn't fully support nested transactions.\n",
      "\n",
      " We'll go with the following:\n",
      "\n",
      "   - engine: created once per test session (session scope)\n",
      "   - tables: created for each test (function scope) and then dropped after the test.\n",
      "\n",
      " But note: the application code already creates the tables (Base.metadata.create_all) when the main module is run.\n",
      " We don't want that to interfere with tests. We are going to override the database URL and the get_db dependency.\n",
      "\n",
      " We are going to:\n",
      "\n",
      "   1. Create a new engine for tests (in-memory) in conftest.py.\n",
      "   2. Create a new SessionLocal for tests.\n",
      "   3. In a fixture (function scope), create the tables and then drop them after the test.\n",
      "\n",
      " However, note that the application code already creates the tables when run. We are not running the application code in the test setup.\n",
      "\n",
      " We are going to import the Base from the application code (which is in the provided code block) and then use it to create the tables.\n",
      "\n",
      " Since the application code is provided as a string, we cannot import it directly. But note: the problem says to output the two files.\n",
      "\n",
      " We are going to assume that the application code is in a module named `main` (if we were to save it as main.py). However, the provided code is in the same file as the test instructions.\n",
      "\n",
      " But note: the problem says to output the two files and nothing else.\n",
      "\n",
      " We are going to write the conftest.py and test_main_with_fixture.py as if the application code is in a module named `main`.\n",
      "\n",
      " However, the provided code is not modularized. It defines the app and the models in the same file.\n",
      "\n",
      " We are going to assume that we are running the tests in the same directory as the main.py file.\n",
      "\n",
      " Alternatively, we can refactor the application code into a module, but the problem says not to change the application code.\n",
      "\n",
      " The problem says: \"Using the application source provided below\"\n",
      "\n",
      " We are going to use the provided application code by including it in the test files? No, that would be duplication.\n",
      "\n",
      " Instead, we are going to assume that the application code is in a file named `main.py` and we are going to import from it.\n",
      "\n",
      " But note: the provided code has a block at the bottom for running with uvicorn. We don't want to run that during tests.\n",
      "\n",
      " We can structure our tests by:\n",
      "\n",
      "   - In conftest.py, we import the necessary components from the main module (which is the application code).\n",
      "\n",
      " However, the problem says: \"Output only the raw Python contents for the two files and nothing else.\"\n",
      "\n",
      " So we write:\n",
      "\n",
      "   === FILE: conftest.py ===\n",
      "   ... code for conftest.py ...\n",
      "\n",
      "   === FILE: test_main_with_fixture.py ===\n",
      "   ... code for test_main_with_fixture.py ...\n",
      "\n",
      " Let's write the conftest.py first.\n",
      "\n",
      " We are going to:\n",
      "\n",
      "   - Import the necessary modules (pytest, FastAPI, TestClient, etc.)\n",
      "   - Import from the main module (which we assume is named `main` and contains the provided code) the Base, app, get_db, etc.\n",
      "\n",
      " Steps:\n",
      "\n",
      "   1. Create a test database engine (in-memory SQLite).\n",
      "   2. Create a TestingSessionLocal class (sessionmaker) bound to the test engine.\n",
      "   3. Create a fixture that creates the tables (Base.metadata.create_all) and then drops them after the test.\n",
      "   4. Create a fixture that returns a database session from TestingSessionLocal and closes it after the test.\n",
      "   5. Override the get_db dependency to use the test session.\n",
      "   6. Create a fixture that returns a TestClient for the app with the overridden dependency.\n",
      "\n",
      " However, note: the application code already has a get_db function that uses the production database.\n",
      "\n",
      " We are going to override `main.get_db` to use our test session.\n",
      "\n",
      " But wait: the application code is provided as a string. We don't have a module named `main` unless we save the provided code as main.py.\n",
      "\n",
      " Since the problem says to use the provided application code, we assume that the code is in a module named `main`.\n",
      "\n",
      " Alternatively, if we are running the tests in the same file, we might have to adjust. But the problem says to generate two files.\n",
      "\n",
      " We are going to write the conftest.py as if we have a module named `main` that contains:\n",
      "\n",
      "   - Base\n",
      "   - app\n",
      "   - get_db\n",
      "   - User, OnboardingTask (if needed)\n",
      "\n",
      " But note: the test_main_with_fixture.py will also import from `main`.\n",
      "\n",
      " However, the problem says: \"generate two Python files for pytest-based isolated DB testing\"\n",
      "\n",
      " We are going to write the two files.\n",
      "\n",
      " Let's code accordingly.\n",
      "\n",
      " Important: We must ensure that the tests are isolated and use the in-memory database.\n",
      "\n",
      " We'll write the conftest.py with the following fixtures:\n",
      "\n",
      "   - test_db_engine: session-scoped, returns the in-memory engine.\n",
      "   - test_db_session: function-scoped, creates the tables and then drops them, yields a session.\n",
      "   - override_get_db: a function that yields the test_db_session.\n",
      "   - client: function-scoped, overrides the get_db dependency and returns a TestClient.\n",
      "\n",
      " However, note: the application code (main) has a get_db function that we want to override.\n",
      "\n",
      " We'll do:\n",
      "\n",
      "   app.dependency_overrides[main.get_db] = override_get_db\n",
      "\n",
      " But we must define override_get_db to yield the test session.\n",
      "\n",
      " Let's write the code for conftest.py.\n",
      "\n",
      " Note: We must also handle the fact that the application code might have already created the tables for the production database.\n",
      " We are not using the production database in tests, so that's okay.\n",
      "\n",
      " We are going to create the tables in the test database (in-memory) for each test and then drop them.\n",
      "\n",
      " Alternatively, we can use a transaction and rollback. But for simplicity, we'll create and drop tables.\n",
      "\n",
      " Since we are using SQLite in-memory, creating and dropping tables is fast.\n",
      "\n",
      " Code for conftest.py:</think>=== FILE: conftest.py ===\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from sqlalchemy.pool import StaticPool\n",
      "\n",
      "from main import app, get_db, Base\n",
      "\n",
      "# Create in-memory SQLite database for testing\n",
      "TEST_DATABASE_URL = \"sqlite:///:memory:\"\n",
      "\n",
      "# Create engine with connect_args for SQLite in-memory\n",
      "test_engine = create_engine(\n",
      "    TEST_DATABASE_URL,\n",
      "    connect_args={\"check_same_thread\": False},\n",
      "    poolclass=StaticPool  # Use static pool for in-memory database\n",
      ")\n",
      "\n",
      "# Create test session factory\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=test_engine)\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def test_db():\n",
      "    \"\"\"\n",
      "    Fixture to create and drop database tables for each test function.\n",
      "    Uses function scope for maximum test isolation.\n",
      "    \"\"\"\n",
      "    # Create all tables\n",
      "    Base.metadata.create_all(bind=test_engine)\n",
      "    yield\n",
      "    # Drop all tables after test\n",
      "    Base.metadata.drop_all(bind=test_engine)\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def db_session(test_db):\n",
      "    \"\"\"\n",
      "    Fixture to provide a database session for each test function.\n",
      "    Automatically rolls back any changes after test completion.\n",
      "    \"\"\"\n",
      "    connection = test_engine.connect()\n",
      "    transaction = connection.begin()\n",
      "    session = TestingSessionLocal(bind=connection)\n",
      "    \n",
      "    yield session\n",
      "    \n",
      "    session.close()\n",
      "    transaction.rollback()\n",
      "    connection.close()\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def client(db_session):\n",
      "    \"\"\"\n",
      "    Fixture to provide a TestClient for FastAPI app with overridden database dependency.\n",
      "    Overrides the get_db dependency to use the test database session.\n",
      "    \"\"\"\n",
      "    def override_get_db():\n",
      "        try:\n",
      "            yield db_session\n",
      "        finally:\n",
      "            pass\n",
      "    \n",
      "    app.dependency_overrides[get_db] = override_get_db\n",
      "    yield TestClient(app)\n",
      "    app.dependency_overrides.clear()\n",
      "\n",
      "=== FILE: test_main_with_fixture.py ===\n",
      "from fastapi.testclient import TestClient\n",
      "\n",
      "def test_create_user_happy_path(client: TestClient):\n",
      "    \"\"\"\n",
      "    Test successful user creation via POST /users/\n",
      "    \"\"\"\n",
      "    user_data = {\n",
      "        \"email\": \"test.user@example.com\",\n",
      "        \"name\": \"Test User\",\n",
      "        \"role\": \"New Hire\"\n",
      "    }\n",
      "    \n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    \n",
      "    assert response.status_code == 200\n",
      "    response_data = response.json()\n",
      "    \n",
      "    assert response_data[\"email\"] == user_data[\"email\"]\n",
      "    assert response_data[\"name\"] == user_data[\"name\"]\n",
      "    assert response_data[\"role\"] == user_data[\"role\"]\n",
      "    assert \"id\" in response_data\n",
      "\n",
      "def test_get_users_happy_path(client: TestClient):\n",
      "    \"\"\"\n",
      "    Test successful retrieval of users via GET /users/\n",
      "    \"\"\"\n",
      "    # First create a user\n",
      "    user_data = {\n",
      "        \"email\": \"test.user@example.com\",\n",
      "        \"name\": \"Test User\",\n",
      "        \"role\": \"New Hire\"\n",
      "    }\n",
      "    create_response = client.post(\"/users/\", json=user_data)\n",
      "    assert create_response.status_code == 200\n",
      "    \n",
      "    # Then get all users\n",
      "    response = client.get(\"/users/\")\n",
      "    assert response.status_code == 200\n",
      "    \n",
      "    users = response.json()\n",
      "    assert len(users) == 1\n",
      "    assert users[0][\"email\"] == user_data[\"email\"]\n",
      "    assert users[0][\"name\"] == user_data[\"name\"]\n",
      "    assert users[0][\"role\"] == user_data[\"role\"]\n",
      "✅ Successfully saved artifact to: tests/conftest.py\n",
      "\n",
      "--- Generating Refactored Tests ---\n",
      "print(\"Hello, World!\")\n",
      "✅ Successfully saved artifact to: tests/test_main_with_fixture.py\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate the pytest fixture for an isolated test database.\n",
    "db_fixture_prompt = f\"\"\" You are a senior Python developer and QA engineer. Using the application source provided below (inserted as {app_code}), generate two Python files for pytest-based isolated DB testing:\n",
    "\n",
    "conftest.py — a professional, well-documented pytest fixture implementation that:\n",
    "\n",
    "Creates an in-memory SQLite engine: sqlite:///:memory: with connect_args={{\"check_same_thread\": False}}.\n",
    "Instantiates a SQLAlchemy Session factory (e.g., SessionLocal = sessionmaker(...)).\n",
    "Creates the DB schema before each test (or for the fixture scope used) and drops it afterward to guarantee a clean state.\n",
    "Provides a fixture that yields a SQLAlchemy Session object and guarantees proper close/teardown.\n",
    "Overrides the FastAPI app's get_db dependency to yield sessions from this test session factory while tests run.\n",
    "Provides a client fixture that returns a fastapi.testclient.TestClient using the app with dependency overrides applied.\n",
    "Uses explicit imports and references from the provided application code (use the app's app instance, DB Base/models, and original get_db name if present; adapt names if they differ in {app_code}).\n",
    "Prefer scope=\"function\" for isolation. If you choose scope=\"session\" explain (in comments only) why and implement nested transaction rollback per-test.\n",
    "Include type hints, concise docstrings, and clear comments for each fixture.\n",
    "test_main_with_fixture.py — a refactor of the \"happy-path\" tests (Challenge 1) so they:\n",
    "\n",
    "Use the client fixture from conftest.py.\n",
    "Test the POST /users/ happy path asserting success status code and response body content.\n",
    "Test the GET /users/ happy path asserting the list/created user is returned as expected.\n",
    "Use only the test client; do not re-create sessions manually.\n",
    "Keep tests deterministic and independent.\n",
    "Output formatting requirements (STRICT):\n",
    "\n",
    "Output only the raw Python contents for the two files and nothing else.\n",
    "Precede each file's contents with a single-line filename marker comment exactly like:\n",
    "=== FILE: conftest.py ===\n",
    "<file contents here> # === FILE: [test_main_with_fixture.py](http://_vscodecontentref_/6) === <file contents here>\n",
    "Do not print prose, explanations, or any additional markers beyond the two file markers and valid Python code.\n",
    "Quality expectations:\n",
    "\n",
    "Working, runnable pytest code that imports from the project's modules found in {app_code}.\n",
    "Uses fastapi.testclient.TestClient and clean dependency override pattern: app.dependency_overrides[get_db] = override_get_db.\n",
    "Proper session lifecycle: create_all before yield, drop_all after yield (or use nested transactions with rollback per test if you document that choice only as inline comments).\n",
    "Clear, maintainable code style and helpful inline comments.\n",
    "Application code context (insert below this line — use it to adapt names and imports): {app_code} \"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# TODO: Write a prompt to refactor the happy path tests to use the new fixture.\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
