{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 1: AI-Generated System Design & Database Seeding (Solution)\n",
    "\n",
    "**Objective:** Use the PRD artifact from Day 1 to generate a detailed SQL database schema, create realistic seed data, and then use those outputs to create and seed a live, local database file.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and Python code for Day 2's first lab. It demonstrates the workflow of generating design artifacts (SQL schema, seed data) and then using code to create a physical database from them.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the `day1_prd.md` artifact from Day 1. This document is the single source of truth for our project's requirements and provides the essential context for the LLM to generate a relevant and accurate database schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the PRD from Day 1\n",
    "prd_content = load_artifact(\"artifacts/day1_prd.md\")\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load day1_prd.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Vision | Image Gen | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| codex-mini-latest | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ✅ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3 | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-V3-Small | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-VL2 | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/DeepSeek-VL2-Small | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/DeepSeek-VL2-Tiny | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\n",
       "| deepseek-ai/Janus-Pro-7B | huggingface | ✅ | ❌ | ❌ | 8,192 | 2,048 |\n",
       "| gemini-2.0-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-live-001 | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ✅ | ✅ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-veo-3 | google | ✅ | ❌ | ❌ | - | - |\n",
       "| google-cloud/speech-to-text/latest_long | google | ❌ | ❌ | ✅ | - | - |\n",
       "| google-cloud/speech-to-text/latest_short | google | ❌ | ❌ | ✅ | - | - |\n",
       "| gpt-4.1 | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-mini | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.5 | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-image-1 | openai | ✅ | ✅ | ❌ | - | - |\n",
       "| imagen-3.0-generate-002 | google | ❌ | ✅ | ❌ | - | - |\n",
       "| imagen-4.0-generate-001 | google | ❌ | ✅ | ❌ | 480 | - |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| whisper-1 | openai | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Vision | Image Gen | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| codex-mini-latest | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| dall-e-3 | openai | ❌ | ✅ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3 | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| deepseek-ai/DeepSeek-V3-Small | huggingface | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| deepseek-ai/DeepSeek-VL2 | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/DeepSeek-VL2-Small | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/DeepSeek-VL2-Tiny | huggingface | ✅ | ❌ | ❌ | 32,000 | 8,000 |\\n| deepseek-ai/Janus-Pro-7B | huggingface | ✅ | ❌ | ❌ | 8,192 | 2,048 |\\n| gemini-2.0-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-live-001 | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ✅ | ✅ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-live-2.5-flash-preview | google | ✅ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-veo-3 | google | ✅ | ❌ | ❌ | - | - |\\n| google-cloud/speech-to-text/latest_long | google | ❌ | ❌ | ✅ | - | - |\\n| google-cloud/speech-to-text/latest_short | google | ❌ | ❌ | ✅ | - | - |\\n| gpt-4.1 | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-mini | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.5 | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-image-1 | openai | ✅ | ✅ | ❌ | - | - |\\n| imagen-3.0-generate-002 | google | ❌ | ✅ | ❌ | - | - |\\n| imagen-4.0-generate-001 | google | ❌ | ✅ | ❌ | 480 | - |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ❌ | ❌ | 200,000 | 100,000 |\\n| tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| whisper-1 | openai | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating the SQL Schema\n",
    "\n",
    "**Explanation:**\n",
    "This prompt instructs the LLM to act as a Database Administrator (DBA). By providing the full PRD as context, we enable the LLM to understand the entities and relationships required by the application. The prompt specifically asks for `CREATE TABLE` statements, guiding the LLM to produce a ready-to-use SQL script. We then clean up the response to remove markdown fences and save the pure SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n",
      "CREATE TABLE users (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    role TEXT CHECK(role IN ('New Hire', 'Manager', 'HR Manager', 'Team Leader')) NOT NULL\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    title TEXT NOT NULL,\n",
      "    description TEXT,\n",
      "    due_date DATE,\n",
      "    status TEXT CHECK(status IN ('Pending', 'Completed')) NOT NULL DEFAULT 'Pending',\n",
      "    user_id INTEGER,\n",
      "    FOREIGN KEY (user_id) REFERENCES users(id)\n",
      ");\n",
      "✅ Successfully saved artifact to: artifacts/schema.sql\n"
     ]
    }
   ],
   "source": [
    "schema_prompt = f\"\"\"\n",
    "You are an expert Database Administrator (DBA).\n",
    "\n",
    "Based on the following Product Requirements Document (PRD), design a normalized SQL schema for a SQLite database. The schema should include tables for users and their assigned onboarding tasks.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "The schema should have at least a `users` table and an `onboarding_tasks` table with a foreign key relationship.\n",
    "- The `users` table should include an id, name, email, and role (e.g., 'New Hire', 'Manager').\n",
    "- The `onboarding_tasks` table should include an id, a title, a description, a due_date, a status (e.g., 'Pending', 'Completed'), and a user_id foreign key.\n",
    "\n",
    "Output only the raw SQL `CREATE TABLE` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    generated_schema = get_completion(schema_prompt, client, model_name, api_provider)\n",
    "    \n",
    "    # Clean up the generated schema\n",
    "    cleaned_schema = clean_llm_output(generated_schema, language='sql')\n",
    "    print(cleaned_schema)\n",
    "    \n",
    "    # Save the cleaned schema to a file\n",
    "    save_artifact(cleaned_schema, \"artifacts/schema.sql\")\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Realistic Seed Data\n",
    "\n",
    "**Explanation:**\n",
    "An empty database isn't very useful for development. This prompt asks the LLM to generate realistic seed data. By providing both the PRD (for thematic context) and the SQL schema (for structural correctness), we guide the LLM to create `INSERT` statements that are both thematically appropriate (e.g., onboarding-related task titles) and syntactically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n",
      "-- Insert sample users\n",
      "INSERT INTO users (name, email, role) VALUES ('Alice Johnson', 'alice.johnson@example.com', 'New Hire');\n",
      "INSERT INTO users (name, email, role) VALUES ('Bob Smith', 'bob.smith@example.com', 'HR Manager');\n",
      "INSERT INTO users (name, email, role) VALUES ('Charlie Brown', 'charlie.brown@example.com', 'Team Leader');\n",
      "\n",
      "-- Insert sample onboarding tasks for Alice Johnson (New Hire)\n",
      "INSERT INTO onboarding_tasks (title, description, due_date, status, user_id) \n",
      "VALUES ('Complete Personal Details', 'Fill out personal details in the HR system', '2023-10-15', 'Pending', 1);\n",
      "\n",
      "INSERT INTO onboarding_tasks (title, description, due_date, status, user_id) \n",
      "VALUES ('Read Company Handbook', 'Read and understand the company handbook', '2023-10-20', 'Pending', 1);\n",
      "\n",
      "INSERT INTO onboarding_tasks (title, description, due_date, status, user_id) \n",
      "VALUES ('Complete Introduction Training', 'Complete the introductory training module', '2023-10-25', 'Pending', 1);\n",
      "\n",
      "-- Insert sample onboarding tasks for Bob Smith (HR Manager)\n",
      "INSERT INTO onboarding_tasks (title, description, due_date, status, user_id) \n",
      "VALUES ('Upload Company Handbook', 'Upload the latest version of the company handbook', '2023-10-10', 'Completed', 2);\n",
      "\n",
      "-- Insert sample onboarding tasks for Charlie Brown (Team Leader)\n",
      "INSERT INTO onboarding_tasks (title, description, due_date, status, user_id) \n",
      "VALUES ('Assign Training Module', 'Assign essential training modules to new hires', '2023-10-12', 'Completed', 3);\n",
      "✅ Successfully saved artifact to: artifacts/seed_data.sql\n"
     ]
    }
   ],
   "source": [
    "seed_data_prompt = f\"\"\"\n",
    "You are a data specialist. Based on the provided PRD and SQL schema, generate 5-10 realistic SQL `INSERT` statements to populate the tables with sample data for an onboarding tool.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "**SQL Schema:**\n",
    "<schema>\n",
    "{cleaned_schema}\n",
    "</schema>\n",
    "\n",
    "Generate at least 3 users and 5 tasks assigned to those users.\n",
    "Output only the raw SQL `INSERT` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "if prd_content and cleaned_schema:\n",
    "    generated_seed_data = get_completion(seed_data_prompt, client, model_name, api_provider)\n",
    "    \n",
    "    # Clean up the generated seed data\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql')\n",
    "    print(cleaned_seed_data)\n",
    "    \n",
    "    # Save the cleaned seed data to a file\n",
    "    save_artifact(cleaned_seed_data, \"artifacts/seed_data.sql\")\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Creating and Seeding a Live Database\n",
    "\n",
    "**Explanation:**\n",
    "This Python function demonstrates a crucial engineering task: turning text-based artifacts into a live system component. The `create_database` function uses Python's built-in `sqlite3` library.\n",
    "1.  It establishes a connection to a database file, which creates the file if it doesn't exist.\n",
    "2.  It reads the `schema.sql` artifact and executes it. It's important to use `cursor.executescript()` here. While `cursor.execute()` is designed for a single SQL statement, `executescript()` is necessary for running a string that contains multiple SQL statements, which is exactly what our `schema.sql` and `seed_data.sql` files contain.\n",
    "3.  It then reads and executes the `seed_data.sql` artifact to populate the newly created tables.\n",
    "4.  `conn.commit()` saves all the changes to the database file.\n",
    "5.  The `finally` block ensures that `conn.close()` is always called, which is a critical best practice to prevent resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing database file at /Users/agaleana/repos/AG-AISOFTDEV/artifacts/onboarding.db\n",
      "Successfully connected to database at /Users/agaleana/repos/AG-AISOFTDEV/artifacts/onboarding.db\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "\n",
    "    # Delete the old database file if it exists to start fresh\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "        print(f\"Removed existing database file at {db_path}\")\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # Read and execute the schema file\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        if schema_sql:\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Read and execute the seed data file if it exists\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "# Define file paths\n",
    "db_file = os.path.join(project_root, \"artifacts\", \"onboarding.db\")\n",
    "schema_file = os.path.join(project_root, \"artifacts\", \"schema.sql\")\n",
    "seed_file = os.path.join(project_root, \"artifacts\", \"seed_data.sql\")\n",
    "\n",
    "# Execute the function\n",
    "create_database(db_file, schema_file, seed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now moved from abstract requirements to a concrete, physical database artifact. You've used an LLM to design a schema, generate realistic test data, and then used a Python script to bring that database to life. This `onboarding.db` file is the foundation upon which we will build our API in Day 3.\n",
    "\n",
    "> **Key Takeaway:** The ability to generate structured data definitions (like a SQL schema) from unstructured text (like a PRD) is a core skill in AI-assisted development. It automates a critical and often time-consuming design step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
